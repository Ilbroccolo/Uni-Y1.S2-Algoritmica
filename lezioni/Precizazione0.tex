% ===================================================================
% FILE: Precizazione0.tex
% ===================================================================
\part*{Precizazione}
\paragraph{Guida alla Complessità Computazionale (Notazione O-Grande)}
La complessità computazionale è un modo per descrivere l'efficienza di un algoritmo.
Non misura il tempo esatto in secondi, ma stima come il numero di operazioni (tempo) o l'uso della memoria (spazio) cresce all'aumentare della dimensione dell'input (indicato con $n$).
La notazione O-grande si concentra sull'ordine di grandezza asintotico, ignorando le costanti moltiplicative e i termini di ordine inferiore.
\begin{definition}[Ordine Asintotico]
L'ordine asintotico descrive come si comporta il tempo (o lo spazio) richiesto da un algoritmo quando la dimensione dell'input ($n$) diventa estremamente grande.
È come guardare la "forma" generale della curva di crescita da molto lontano.

Si ignorano i dettagli iniziali e la ripidità iniziale della curva, per concentrarci unicamente sul termine che cresce più velocemente, visto che sarà il più impattante quando $n$ sarà enorme.
Ad esempio, se un algoritmo impiega $3n^2 + 10n + 5$ operazioni, il suo ordine asintotico è $O(n^2)$.

Perché?
Perché quando $n$ diventa grandissimo (es. un milione), il termine $n^2$ è talmente più grande di $n$ e di $5$ che gli altri diventano irrilevanti per capire l'andamento generale.
\end{definition}

\section{Differenza Chiave: $O(n)$ (Lineare) vs. $O(\log n)$ (Logaritmico)}
Spesso si crea confusione tra un costo come $n/2$ e uno come $\log n$, ma appartengono a due universi di efficienza completamente diversi.
\begin{itemize}
    \item \textbf{Lineare $O(n)$:} Un algoritmo con un costo proporzionale a $n$ (come $n$, $n/2$ o $2n$) ha una complessità Lineare, $O(n)$.
    Questo significa che il numero di operazioni è direttamente proporzionale alla dimensione dell'input.
    Se l'input raddoppia, anche il tempo di esecuzione (circa) raddoppia. Nella notazione O-grande, le costanti (come $1/2$) vengono ignorate.
    Ad esempio, la ricerca lineare in un array non ordinato richiede, nel caso medio, $n/2$ controlli.
    La sua complessità è comunque $O(n)$.
    
    \item \textbf{Logaritmico $O(\log n)$:} Un algoritmo con costo $\log n$ (logaritmo in base 2, $\log_2 n$) ha una complessità Logaritmica, $O(\log n)$.
    Questo tipo di algoritmo è estremamente efficiente perché, ad ogni passo, è in grado di scartare una frazione significativa del problema (di solito la metà).
    L'esempio classico è la ricerca binaria.
\end{itemize}

\begin{example}[Confronto Pratico: $O(n)$ vs $O(\log n)$]
Su un input di $n = 1.000.000$ di elementi:
\begin{itemize}
    \item Un algoritmo lineare ($n/2$) richiederebbe circa \textbf{500.000} operazioni.
    \item Un algoritmo logaritmico ($\log_2 n$) ne richiederebbe circa \textbf{20}.
\end{itemize}
$O(\log n)$ è drasticamente più veloce di $O(n)$.
\end{example}

\section{Caso Pessimo, Medio e Ottimo}
La performance di un algoritmo può cambiare non solo in base alla dimensione dell'input ($n$), ma anche in base a come è fatto l'input.
\begin{observation}[Caso Pessimo, Medio e Ottimo] \\
\begin{itemize}
    \item \textbf{Caso Pessimo (Worst Case):} Rappresenta lo scenario che richiede il massimo numero di operazioni.
    È l'input "peggiore" possibile. È la metrica più importante e quasi sempre quella che si utilizza, perché fornisce una garanzia sulla performance: l'algoritmo non farà mai peggio di così.
    \item \textbf{Caso Medio (Average Case):} Descrive la performance "tipica" calcolata come media su tutti i possibili input.
    \item \textbf{Caso Ottimo (Best Case):} Descrive lo scenario più veloce in assoluto (ma spesso poco utile, perché si verifica solo in condizioni molto specifiche).
\end{itemize}
\end{observation}

\section{Classi di Complessità (dal più veloce al più lento)}
\begin{example}[Gerarchia delle Complessità]
\begin{description}
    \item[$O(1)$ - Costante:] Il tempo non dipende da $n$. (Es. Accesso a un array `array[i]`).
    \item[$O(\log n)$ - Logaritmico:] Il tempo cresce molto lentamente. (Es. Ricerca binaria).
    \item[$O(n)$ - Lineare:] Il tempo cresce linearmente con $n$. (Es. Un singolo ciclo for, trovare il massimo).
    \item[$O(n \log n)$ - Linearitmico:] Ottima complessità per gli algoritmi di ordinamento. (Es. Merge Sort, Heapsort).
    \item[$O(n^2)$ - Quadratico:] Il tempo cresce con il quadrato di $n$. (Es. Due cicli for annidati, Bubble Sort, Insertion Sort).
    \item[$O(n^k)$ - Polinomiale:] Il tempo cresce con $n$ elevato a una costante $k$. (Es. Tre cicli annidati $O(n^3)$).
    \item[$O(2^n)$ - Esponenziale:] Diventa intrattabile molto rapidamente. (Es. Soluzioni "brute force" che provano tutte le combinazioni).
    \item[$O(n!)$ - Fattoriale:] Il peggior caso possibile. (Es. "Brute force" al problema del commesso viaggiatore).
\end{description}
\end{example}

\section{Regole di Calcolo (Come Combinare)}
Per calcolare la complessità di un programma intero, si combinano i costi delle sue parti usando due regole fondamentali.
\begin{observation}[Regole di Calcolo Asintotico]
\begin{itemize}
    \item \textbf{Regola della Somma (Operazioni in sequenza):} Se hai un blocco A seguito da un blocco B, la complessità totale è $O(A) + O(B)$.
    Si tiene solo il termine dominante.
    Ad esempio, un ciclo $O(n)$ seguito da due cicli annidati $O(n^2)$ ha una complessità totale $O(n) + O(n^2)$, che si semplifica in $O(n^2)$.
    \item \textbf{Regola del Prodotto (Operazioni annidate):} Se un blocco B è all'interno di un blocco A, le complessità si moltiplicano: $O(A) \times O(B)$.
    L'esempio classico è un `for` $O(n)$ che contiene un altro `for` $O(n)$: la complessità totale è $O(n \times n) = O(n^2)$.
\end{itemize}
\end{observation}

\section{Impatto dell'Ordinamento: Array Ordinato vs. Non Ordinato}
Avere un array di input già ordinato (o decidere di ordinarlo) può cambiare drasticamente la complessità.
L'operazione di ordinamento in sé ha un costo, tipicamente $O(n \log n)$.
\begin{example}[Ricerca di un elemento: $O(n)$ vs $O(\log n)$]
\begin{itemize}
    \item \textbf{Non Ordinato:} Ricerca lineare (controllarli uno per uno).
    Caso pessimo: $O(n)$.
    \item \textbf{Ordinato:} Ricerca binaria (dimezzando l'intervallo). Caso pessimo: $O(\log n)$.
\end{itemize}
\end{example}

\begin{example}[Ricerca di duplicati: $O(n^2)$ vs $O(n)$]
\begin{itemize}
    \item \textbf{Non Ordinato:} Confrontare ogni elemento con ogni altro.
    Caso pessimo: $O(n^2)$.
    \item \textbf{Ordinato:} Basta una singola scansione lineare. Se $A[i] == A[i+1]$, esiste un duplicato. Caso pessimo: $O(n)$.
\end{itemize}
\end{example}

\begin{example}[Trovare il minimo o il massimo: $O(n)$ vs $O(1)$]
\begin{itemize}
    \item \textbf{Non Ordinato:} Bisogna scorrere tutto l'array.
    Caso pessimo: $O(n)$.
    \item \textbf{Ordinato:} Il minimo è il primo elemento ($A[0]$) e il massimo è l'ultimo ($A[n-1]$).
    Caso pessimo: $O(1)$.
\end{itemize}
\end{example}

\subsection{Quando ha senso ordinare l'array?}
Ordinare (costo $O(n \log n)$) ha senso quando il costo totale è inferiore a quello dell'operazione sull'array non ordinato.
\begin{example}[Scenario 1: Operazione singola (Trova max)]
\begin{itemize}
    \item \textbf{Costo (Non ordinato):} $O(n)$.
    \item \textbf{Costo (Ordinando prima):} $O(n \log n) \text{ (sort)} + O(1) \text{ (accesso)} = O(n \log n)$.
    \item \textbf{Verdetto:} $O(n)$ è molto meglio. Non ordinare.
\end{itemize}
\end{example}

\begin{example}[Scenario 2: Operazioni multiple (k ricerche)]
Se devi effettuare $k$ ricerche diverse su $n$ elementi.
\begin{itemize}
    \item \textbf{Costo (Non ordinato):} $k$ ricerche lineari $\implies k \times O(n) = O(k \cdot n)$.
    \item \textbf{Costo (Ordinando prima):} $O(n \log n) \text{ (una tantum)} + k \times O(\log n) \implies O(n \log n + k \log n)$.
    \item \textbf{Verdetto:} Se $k$ è grande (es. $k \approx O(n)$), il costo $O(n \log n)$ è drasticamente migliore di $O(n^2)$.
    Ha senso ordinare.
\end{itemize}
\end{example}

\section{Complessità Spaziale (Cenno)}
Oltre al tempo, la complessità spaziale misura quanta memoria ausiliaria (spazio extra oltre all'input) usa l'algoritmo.
Può essere $O(1)$ (costante), se usa solo un numero fisso di variabili, o $O(n)$ (lineare), se ha bisogno di creare una struttura dati (come un array di supporto) grande quanto l'input.
\section{Esempi di Analisi (Linguaggio MAO)}
Analizziamo il costo (caso pessimo) di alcuni frammenti di codice MAO.
% --- NOTA: Rimuovo il \newcommand{\mao} che era rotto e non usato.
% Uso direttamente l'ambiente 'listings' come hai fatto.
\begin{example}[Esempio 1: Ciclo Singolo (Lineare)]
\begin{lstlisting}[language={}, basicstyle=\ttfamily\bfseries, backgroundcolor=\color{codegray}, frame=tb, numbers=none, breaklines=true, tabsize=2]
int i=0;
int s=0;
while (i < n) {
    s := s + i;
    i := i + 1;
}
\end{lstlisting}
\textbf{Analisi ($O(n)$):} Il codice esegue due comandi iniziali $O(1)$. Segue un ciclo `while`.
Il corpo del ciclo ha costo costante $O(1)$. La guardia viene valutata $n+1$ volte e il corpo viene eseguito $n$ volte.
La complessità totale è (Regola della Somma): $O(1) + O(n \times 1)$. Il termine dominante è $O(n)$.
\end{example}

\begin{example}[Esempio 2: Cicli Annidati (Quadratico)]
\begin{lstlisting}[language={}, basicstyle=\ttfamily\bfseries, backgroundcolor=\color{codegray}, frame=tb, numbers=none, breaklines=true, tabsize=2]
int i=0;
int r=0;
while (i < n) {
    int j=0;
while (j < n) {
        r := r + 1;
j := j + 1;
    }
    i := i + 1;
}
\end{lstlisting}
\textbf{Analisi ($O(n^2)$):} Abbiamo due cicli annidati.
Il ciclo esterno (while $i<n$) esegue il suo corpo $n$ volte.
Il corpo contiene un ciclo interno (while $j<n$) che viene eseguito $n$ volte per ogni iterazione esterna.
Per la Regola del Prodotto, la complessità è $O(n \times n) = O(n^2)$.
\end{example}

\begin{example}[Esempio 3: Sequenza di Cicli (Regola della Somma)]
\begin{lstlisting}[language={}, basicstyle=\ttfamily\bfseries, backgroundcolor=\color{codegray}, frame=tb, numbers=none, breaklines=true, tabsize=2]
int s=0;
int i=0;
while (i < n) {
    s := s + i;
    i := i + 1;
}
int j=0;
while (j < n) {
    int k=0;
while (k < n) {
        s := s + 1;
k := k + 1;
    }
    j := j + 1;
}
\end{lstlisting}
\textbf{Analisi ($O(n^2)$):} Questo codice è una sequenza di due blocchi.
\begin{itemize}
    \item Il primo blocco è un ciclo singolo: costo $O(n)$.
    \item Il secondo blocco è composto da due cicli annidati: costo $O(n^2)$.
\end{itemize}
Per la Regola della Somma, la complessità totale è $O(n) + O(n^2)$.
Si considera solo il termine dominante, quindi la complessità è $O(n^2)$.
\end{example}

\begin{example}[Esempio 4: Ciclo Logaritmico]
\begin{lstlisting}[language={}, basicstyle=\ttfamily\bfseries, backgroundcolor=\color{codegray}, frame=tb, numbers=none, breaklines=true, tabsize=2]
int i=1;
while (i < n) {
    skip;
i := i * 2;
}
\end{lstlisting}
\textbf{Analisi ($O(\log n)$):} La variabile di controllo $i$ non viene incrementata linearmente ($i+1$), ma viene moltiplicata per 2. I valori di $i$ saranno $1, 2, 4, 8, 16, ..., 2^k$ fino a superare $n$.
Il numero di iterazioni ($k$) è il più piccolo intero tale che $2^k \ge n$.
Questo $k$ è esattamente $\log_2 n$. Poiché il corpo del ciclo ha costo $O(1)$, la complessità totale è $O(\log n)$.
\end{example}

\begin{example}[Esempio 5: Condizionale nel Caso Pessimo]
\begin{lstlisting}[language={}, basicstyle=\ttfamily\bfseries, backgroundcolor=\color{codegray}, frame=tb, numbers=none, breaklines=true, tabsize=2]
int i=0;
int r=0;
while (i < n) {
    if (i < 10) {
        r := r + 1;
// Costo O(1)
    } else {
        int j=0;
while (j < n) {
            r := r + j;
// Costo O(n)
            j := j + 1;
}
    }
    i := i + 1;
}
\end{lstlisting}
\textbf{Analisi ($O(n^2)$):} Stiamo analizzando il caso pessimo.
Il ciclo esterno (while $i<n$) viene eseguito $n$ volte. All'interno c'è un `if`.
Dobbiamo considerare il costo del ramo più pesante.
\begin{itemize}
    \item Il ramo `if` ($i<10$) ha costo $O(1)$.
    \item Il ramo `else` contiene un ciclo lineare, costo $O(n)$.
\end{itemize}
Nell'analisi del caso pessimo, assumiamo che venga sempre eseguito il ramo più costoso.
Quasi tutte le iterazioni (per $i \ge 10$) eseguiranno il ramo `else`, che costa $O(n)$.
Applicando la Regola del Prodotto, abbiamo il ciclo esterno $O(n)$ che contiene un blocco che (nel caso peggiore) costa $O(n)$.
La complessità totale è $O(n \times n) = O(n^2)$.
\end{example}

\newpage